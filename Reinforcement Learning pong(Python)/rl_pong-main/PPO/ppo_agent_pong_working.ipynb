{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d320e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym \n",
    "!pip install 'gym[box2d]'\n",
    "!pip install atari_py\n",
    "!pip install ale-py\n",
    "!pip install \"gym[atari, accept-rom-license]\"\n",
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install matplotlib.\n",
    "!pip install opencv-python\n",
    "!pip install gym[atari]\n",
    "!pip install torch torchvision\n",
    "!pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44651c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_state(state):\n",
    "    state = state[35:195, :, :]  # crop the screen\n",
    "    state = state[::2, ::2, :]  # downsample by a factor of 2\n",
    "    state = state.mean(axis=2)  # convert to grayscale\n",
    "    state = state / 255.0  # normalize\n",
    "    return torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        self._flat_size = self._get_flat_size()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self._flat_size, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Linear(512, num_actions)\n",
    "        self.critic = nn.Linear(512, 1)\n",
    "\n",
    "    def _get_flat_size(self):\n",
    "        x = torch.zeros(1, 1, 80, 80)\n",
    "        x = self.conv(x)\n",
    "        return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        action_prob = torch.softmax(self.actor(x), dim=-1)\n",
    "        value = self.critic(x)\n",
    "        return action_prob, value\n",
    "\n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids], actions[rand_ids], log_probs[rand_ids], returns[rand_ids], advantage[rand_ids]\n",
    "\n",
    "\n",
    "def ppo_update(model, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            action_prob, value = model(state)\n",
    "            dist = Categorical(action_prob)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            (actor_loss + 0.5 * critic_loss - 0.001 * entropy).backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "def compute_gae(next_value, rewards, masks, values, gamma, tau):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66cd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the original state (frame) from the environment\n",
    "env = gym.make('ALE/Pong-v5')\n",
    "state = env.reset()\n",
    "\n",
    "# Preprocess the state using the provided preprocessing function\n",
    "preprocessed_state = preprocess_state(state)\n",
    "\n",
    "# Convert the preprocessed PyTorch tensor to a PIL Image\n",
    "preprocessed_image = TF.to_pil_image(preprocessed_state.squeeze(0).squeeze(0))\n",
    "\n",
    "# Display the preprocessed image\n",
    "plt.imshow(preprocessed_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beafce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model, save_path='models', model_name='ppo_pong.pt'):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    torch.save(model.state_dict(), os.path.join(save_path, model_name))\n",
    "    print(f\"Model saved as {os.path.join(save_path, model_name)}\")\n",
    "\n",
    "def ppo(env_name='ALE/Pong-v5', num_actions=3, num_epochs=10000, num_steps=1000,\n",
    "        mini_batch_size=64, ppo_epochs=4, gamma=0.99, tau=0.95,\n",
    "        lr=1e-3, clip_param=0.2, log_interval=10, save_interval=100):\n",
    "    \n",
    "    training_rewards = []\n",
    "    logged_rewards = []\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = ActorCritic(num_actions).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    action_mapping = {\n",
    "        0: 0,\n",
    "        1: 2,\n",
    "        2: 3\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state).to(device)\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        actions = []\n",
    "        states = []\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            action_prob, value = model(state)\n",
    "            dist = Categorical(action_prob)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action_mapping[action.item()])\n",
    "            next_state = preprocess_state(next_state).to(device)\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1 - done], dtype=torch.float, device=device))\n",
    "            actions.append(action)\n",
    "            \n",
    "            states.append(state) \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        next_value = model(state)[1]\n",
    "        returns = compute_gae(next_value, rewards, masks, values, gamma, tau)\n",
    "        \n",
    "\n",
    "        returns = torch.cat(returns).detach()\n",
    "        log_probs = torch.cat(log_probs).detach()\n",
    "        values = torch.cat(values).detach()\n",
    "        actions = torch.cat(actions)\n",
    "        states  = torch.cat(states)\n",
    "        advantages = returns - values\n",
    "        \n",
    "        ppo_update(model, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param)\n",
    "\n",
    "        training_rewards.append(torch.sum(torch.tensor(rewards)).item())\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            print(f'Epoch: {epoch}, Reward: {np.sum(rewards)}')\n",
    "            logged_rewards.append(np.sum(rewards))\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            save_model(model)\n",
    "            \n",
    "    \n",
    "    #plt.plot(training_rewards)\n",
    "    #plt.xlabel('Epoch')\n",
    "    #plt.ylabel('Reward')\n",
    "    #plt.title('Training Rewards per Epoch')\n",
    "    #plt.show()\n",
    "        \n",
    "    return model, logged_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trained_model, logged_rewards = ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee13c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    padding = np.zeros(window_size - 1)\n",
    "    data_padded = np.concatenate((padding, data))\n",
    "    cumsum = np.cumsum(data_padded)\n",
    "    return (cumsum[window_size:] - cumsum[:-window_size]) / float(window_size)\n",
    "\n",
    "window_size = 5 # Adjust this value to your preference\n",
    "smoothed_rewards = moving_average(logged_rewards, window_size)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(0, len(smoothed_rewards2) * 10, 10), smoothed_rewards2)\n",
    "plt.xlabel('Eposide')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Smoothed Training Rewards per Episode')\n",
    "plt.show("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_model(trained_model, env_name='ALE/Pong-v5', num_episodes=500):\n",
    "    env = gym.make(env_name)\n",
    "    device = torch.device(\"cpu\")\n",
    "    rewards = []\n",
    "\n",
    "    action_mapping = {\n",
    "        0: 0,\n",
    "        1: 2,\n",
    "        2: 3\n",
    "    }\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state).to(device)\n",
    "        episode_reward = 0\n",
    "\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                action_prob, _ = trained_model(state)\n",
    "                dist = Categorical(action_prob)\n",
    "                action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action_mapping[action.item()])\n",
    "            state = preprocess_state(next_state).to(device)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(episode_reward)\n",
    "                print(f'Episode: {episode}, Reward: {episode_reward}')\n",
    "                break\n",
    "\n",
    "    return rewards\n",
    "\n",
    "evaluation_rewards = evaluate_trained_model(trained_model)\n",
    "window_size = 5  # Adjust the window size for the moving average\n",
    "smoothed_evaluation_rewards = moving_average(evaluation_rewards, window_size)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(smoothed_evaluation_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Evaluation Rewards per Episode')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
